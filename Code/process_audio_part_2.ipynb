{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall kiwisolver matplotlib\n",
    "# !pip install kiwisolver matplotlib\n",
    "# !pip install tensorflow[and-cuda]==2.10\n",
    "# !pip list | grep cud\n",
    "# import tensorflow as tf\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "# !pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8229179",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries ###\n",
    "import os, sys, re, time, json, traceback, logging, datetime, gc, shutil, math, base64, pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "# from numba import cuda\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "### Audio Libraries ###\n",
    "from pydub import AudioSegment, effects\n",
    "from pyannote.audio import Pipeline\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier, SpeakerRecognition\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57db005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "# import tensorflow as tf\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path of Files ###\n",
    "input_video_filename_wo_ext = '1_min_video'\n",
    "input_video_filename = 'Videos_Examples/'+ input_video_filename_wo_ext +'.mp4' # The video files should be on mp4 format\n",
    "output_video_path = 'Videos_Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory of output of videos\n",
    "if not os.path.exists(output_video_path):\n",
    "    os.makedirs(output_video_path)\n",
    "\n",
    "# Create a new directory for processed audio if it does not exist\n",
    "dir_output_audio = output_video_path + input_video_filename_wo_ext +'/Audio/'\n",
    "if not os.path.exists(dir_output_audio):\n",
    "    os.makedirs(dir_output_audio)\n",
    "dir_output_audio_step1 = output_video_path + input_video_filename_wo_ext +'/Audio/Step1_ProcessAudio/'\n",
    "if not os.path.exists(dir_output_audio_step1):\n",
    "    os.makedirs(dir_output_audio_step1)\n",
    "dir_output_audio_step2 = output_video_path + input_video_filename_wo_ext +'/Audio/Step2_Spleeter/'\n",
    "if not os.path.exists(dir_output_audio_step2):\n",
    "    os.makedirs(dir_output_audio_step2)\n",
    "dir_output_audio_step3 = output_video_path + input_video_filename_wo_ext +'/Audio/Step3_Segments/'\n",
    "if not os.path.exists(dir_output_audio_step3):\n",
    "    os.makedirs(dir_output_audio_step3)\n",
    "dir_output_audio_step4 = output_video_path + input_video_filename_wo_ext +'/Audio/Step4_Embeddings/'\n",
    "if not os.path.exists(dir_output_audio_step4):\n",
    "    os.makedirs(dir_output_audio_step4)\n",
    "dir_output_audio_step5 = output_video_path + input_video_filename_wo_ext +'/Audio/Step5_Clusters/'\n",
    "if not os.path.exists(dir_output_audio_step5):\n",
    "    os.makedirs(dir_output_audio_step5)\n",
    "    \n",
    "# Create a new directory for processed images if it does not exist\n",
    "dir_output_images = output_video_path + input_video_filename_wo_ext +'/Images/'\n",
    "if not os.path.exists(dir_output_images):\n",
    "    os.makedirs(dir_output_images)\n",
    "dir_output_images_step1 = output_video_path + input_video_filename_wo_ext +'/Images/Step1_Images/'\n",
    "if not os.path.exists(dir_output_images_step1):\n",
    "    os.makedirs(dir_output_images_step1)\n",
    "dir_output_images_step2 = output_video_path + input_video_filename_wo_ext +'/Images/Step2_Faces/'\n",
    "if not os.path.exists(dir_output_images_step2):\n",
    "    os.makedirs(dir_output_images_step2)\n",
    "dir_output_images_step3 = output_video_path + input_video_filename_wo_ext +'/Images/Step3_Embeddings/'\n",
    "if not os.path.exists(dir_output_images_step3):\n",
    "    os.makedirs(dir_output_images_step3)\n",
    "dir_output_images_step4 = output_video_path + input_video_filename_wo_ext +'/Images/Step4_Clusters/'\n",
    "if not os.path.exists(dir_output_images_step4):\n",
    "    os.makedirs(dir_output_images_step4)\n",
    "\n",
    "# Create a new directory for processed text if it does not exist\n",
    "dir_output_text = output_video_path + input_video_filename_wo_ext +'/Text/'\n",
    "if not os.path.exists(dir_output_text):\n",
    "    os.makedirs(dir_output_text)\n",
    "dir_output_text_seg = output_video_path + input_video_filename_wo_ext +'/Text/Segments/'\n",
    "if not os.path.exists(dir_output_text_seg):\n",
    "    os.makedirs(dir_output_text_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Video to Audio\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=\"\")\n",
    "# pipeline = pipeline.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "### Segments Separation ###\n",
    "# Apply pretrained pipeline\n",
    "diarization = pipeline(dir_output_audio_step2 + 'audio.flac')\n",
    "timeline = list()\n",
    "speakers_dict = dict()\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    timeline.append([speaker.replace('SPEAKER_', 'SP'), round(turn.start,1), round(turn.end,1)])\n",
    "    speakers_dict[speaker] = 1\n",
    "\n",
    "# Export speech part\n",
    "audio_file = AudioSegment.from_file(file=dir_output_audio_step2 + 'audio.flac', format=\"flac\")\n",
    "\n",
    "n_clusters = len(speakers_dict)\n",
    "\n",
    "count = 0\n",
    "for seg in timeline:\n",
    "    output_audio = audio_file[seg[1]*1000:seg[2]*1000]\n",
    "    secs = seg[2]-seg[1]\n",
    "    if secs > 1:\n",
    "        output_audio.export(str(dir_output_audio_step3 + 'seg_' + str(count) +'_'+ seg[0] +'_' + str(seg[1]) + '-' + str(seg[2]) +'.flac'), format=\"flac\")\n",
    "        count += 1\n",
    "\n",
    "# pipeline = None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()\n",
    "print('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53882683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.audio import Inference, Model, Audio\n",
    "# from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "# from speechbrain.pretrained import EncoderClassifier, SpeakerRecognition\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")\n",
    "# model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\", device=torch.device(\"cuda\"))\n",
    "# audio = Audio(sample_rate=16000, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011476eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "### Create Voide Embeddings ###\n",
    "# if not os.path.exists(dir_output_audio_step4 + 'embeddings.pkl'): # Create embeddings\n",
    "embeddings = []\n",
    "embeddings_filename = []\n",
    "\n",
    "for filename in sorted(os.listdir(dir_output_audio_step3)):\n",
    "    if '.flac' in filename:\n",
    "        #embeddings.append(DeepFace.represent(dir_output_audio_step3+filename, enforce_detection=False)[0]['embedding'])\n",
    "        signal, fs = torchaudio.load(dir_output_audio_step3+filename)\n",
    "        embeddings.append(classifier.encode_batch(signal, normalize=True)[0][0])\n",
    "        embeddings_filename.append(filename)\n",
    "\n",
    "# Store pickle/embeddings\n",
    "embeddings_all = {'embeddings': embeddings, 'filename': embeddings_filename}\n",
    "\n",
    "f_pickle = open(dir_output_audio_step4 + 'embeddings.pkl', \"wb\")\n",
    "pickle.dump(embeddings_all, f_pickle)\n",
    "f_pickle.close()\n",
    "\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pickle = open(dir_output_audio_step4 + 'embeddings.pkl', 'rb')\n",
    "embeddings_all = pickle.load(f_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# clf = DBSCAN(n_jobs=-1, min_samples=1, eps=42, metric='euclidean') # cosine (0.000199) euclidean\n",
    "# clf.fit(embeddings_all['embeddings'])\n",
    "# preds = clf.labels_\n",
    "# print(preds)\n",
    "# print('Number of clusters:', len(set(preds)))\n",
    "\n",
    "# # Calculate silhouette score to evaluate the quality of clustering\n",
    "# silhouette_avg = silhouette_score(embeddings_all['embeddings'], preds)\n",
    "# print(\"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "### Cluster Voice Embeddings ###\n",
    "# Delete all clusters folders\n",
    "for filename in sorted(os.listdir(dir_output_audio_step5)):\n",
    "    if 'cluster' in filename:\n",
    "        shutil.rmtree(dir_output_audio_step5+filename)\n",
    "\n",
    "# Dimensionality reduction for visualization and potentially better clustering\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_all['embeddings'])\n",
    "        \n",
    "# clf = KMeans(n_clusters=n_clusters, n_init=100, max_iter=600, random_state=0)\n",
    "# if kmeans use number of speakers from voice\n",
    "clf = DBSCAN(n_jobs=-1, min_samples=2, eps=5.5, metric='euclidean')\n",
    "# cosine (0.000025), (min_samples=2, eps=5.5, metric='euclidean'), (min_samples=3, eps=0.01, metric='cosine')\n",
    "\n",
    "# clf.fit(embeddings_all['embeddings'])\n",
    "clf.fit(reduced_embeddings)\n",
    "preds = clf.labels_\n",
    "print(preds)\n",
    "print('Number of clusters:', len(set(preds)))\n",
    "\n",
    "for i in range(0, len(preds)):\n",
    "    cluster_dir = 'cluster ' + str(preds[i]) + '/'\n",
    "\n",
    "    if not os.path.exists(dir_output_audio_step5 + cluster_dir):\n",
    "        os.makedirs(dir_output_audio_step5 + cluster_dir)\n",
    "    \n",
    "    shutil.copyfile(dir_output_audio_step3 + embeddings_all['filename'][i], dir_output_audio_step5 + cluster_dir + embeddings_all['filename'][i])\n",
    "\n",
    "# Calculate silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(embeddings_all['embeddings'], preds)\n",
    "print(\"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58412af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "# %%timeit\n",
    "### Cluster Voice Embeddings ###\n",
    "# Delete all clusters folders\n",
    "for filename in sorted(os.listdir(dir_output_audio_step5)):\n",
    "    if 'cluster' in filename:\n",
    "        shutil.rmtree(dir_output_audio_step5+filename)\n",
    "\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "clusters_list_i = []\n",
    "for num_clusters in range(6, 13):  # testing number of clusters from 1 to 10\n",
    "    clusters_list_i.append(num_clusters)\n",
    "    # KMeans Clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init='auto', max_iter=2000, random_state=42)\n",
    "    kmeans.fit(embeddings_all['embeddings'])\n",
    "    labels_clusters = kmeans.fit_predict(embeddings_all['embeddings'])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(embeddings_all['embeddings'], labels_clusters))\n",
    "\n",
    "# Assuming 'wcss' is a list of your Within-Cluster-Sum-of-Squares values and 'range_n_clusters' is the list of number of clusters\n",
    "knee_locator = KneeLocator(clusters_list_i, wcss, curve='convex', direction='decreasing')\n",
    "optimal_num_clusters = knee_locator.elbow\n",
    "print('Optimal Clusters:', optimal_num_clusters)\n",
    "\n",
    "# Train final Kmeans using the optimal number of clusters\n",
    "clf = KMeans(n_clusters=optimal_num_clusters, init='k-means++', n_init='auto', max_iter=2000, random_state=42)\n",
    "clf.fit(embeddings_all['embeddings'])\n",
    "# labels_clusters = clf.fit_predict(embeddings_all['embeddings'])\n",
    "preds = clf.labels_\n",
    "\n",
    "# clf = KMeans(n_clusters=n_clusters, n_init=100, max_iter=600, random_state=0)\n",
    "# clf.fit(embeddings_all['embeddings'])\n",
    "# preds = clf.labels_\n",
    "# print(preds)\n",
    "# print('Number of clusters:', len(set(preds)))\n",
    "\n",
    "# Copy to correct cluster\n",
    "for i in range(0, len(preds)):\n",
    "    cluster_dir = 'cluster ' + str(preds[i]) + '/'\n",
    "    if not os.path.exists(dir_output_audio_step5 + cluster_dir):\n",
    "        os.makedirs(dir_output_audio_step5 + cluster_dir)\n",
    "    shutil.copyfile(dir_output_audio_step3 + embeddings_all['filename'][i], dir_output_audio_step5 + cluster_dir + embeddings_all['filename'][i])\n",
    "\n",
    "print('Finish.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
